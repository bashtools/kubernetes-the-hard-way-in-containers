.TS # KTHW 09 Bootstrapping Kubernetes Workers
.TS 
.TS ![](../images/kthw-9.gif)
.TS
.TS View the [screencast file](../screencasts/kthw-9.scr)
.TS
.TS ```
.MD >  **Kubernetes the Hard Way using My Own Kind**
.MD > 
.MD > View a [screencast and transcript](kthw-9-transcript.md)
screencast delay 10
screencast clear
screencast paste
# ---------------------------------------------------------
screencast paste
# Kubernetes the Hard Way - using `mokctl` from My Own Kind
screencast paste
# ---------------------------------------------------------
screencast paste
# 09-bootstrapping-kubernetes-workers
screencast paste
# Configure and start the kubernetes workers

.MD # Bootstrapping the Kubernetes Worker Nodes
.MD 
.MD In this lab you will bootstrap three Kubernetes worker nodes. The following components will be installed on each node: [runc](https://github.com/opencontainers/runc), [container networking plugins](https://github.com/containernetworking/cni), [containerd](https://github.com/containerd/containerd), [kubelet](https://kubernetes.io/docs/admin/kubelet), and [kube-proxy](https://kubernetes.io/docs/concepts/cluster-administration/proxies).
.MD 
.MD ## Prerequisites
.MD 
.MD The commands in this lab must be run on each worker instance: `kthw-worker-1`, `kthw-worker-2`, and `kthw-worker-3`. Log in to each controller instance using the `mokctl` command. Example:
.MD 
.MD ```
.MD mokctl exec kthw-worker-1
.MD ```
.MD 
.MD ### Running commands in parallel with tmux
.MD 
.MD [tmux](https://github.com/tmux/tmux/wiki) can be used to run commands on multiple compute instances at the same time. See the [Running commands in parallel with tmux](01-prerequisites.md#running-commands-in-parallel-with-tmux) section in the Prerequisites lab.
.MD 
.MD ## Provisioning a Kubernetes Worker Node
.MD 
.MD Before logging in to the workers we need to copy the cluster-list to each worker node:
.MD
.MD ```
# The cluster list needs to be copied over to be able to get the IPs of all the nodes
# Copy to the masters
screencast paste start
for instance in kthw-worker-1 kthw-worker-2 kthw-worker-3; do
  sudo docker cp kthw-certs/cluster-list.txt ${instance}:/root/
done
screencast paste end
.MD ```
.MD 
# Provision the Kubernetes Worker Nodes

screencast prompt %$#
# We will use 'tmux' to log in to three containers and then
# execute the commands in parallel.
screencast delay 18

.MD Use `tmux` to log in to all three worker at the same time:
.MD
.MD ```
screencast prompt %$#1234567890
tmux
tmux set status off
screencast prompt %$#
tmux split
tmux split
tmux select-layout even-vertical
tmux select-pane -D
screencast sleep 1
mokctl exec kthw-worker-1
screencast sleep 1
^aj
screencast sleep 1
mokctl exec kthw-worker-2
screencast sleep 1
^aj
screencast sleep 1
tmux select-layout tiled
tmux resize-pane -y 25
mokctl exec kthw-worker-3
# syncing screens
screencast sleep 1
^a^x
screencast sleep 1
# Panes are now synced!
# Clearing the screen
clear
.MD ```
.MD
.MD `mokctl` installed a few kubernetes services ready for set up.
.MD
.MD Ensure all existing kubernetes services are deleted:
.MD
.MD ```
# Remove the existing k8s services:
screencast paste
yum -y remove kubelet kubeadm cri-o cri-tools runc criu
.MD ```
.MD
.MD Change to root's home directory, where the certs are:
.MD
.MD ```
cd # <- All our certs are in root's home
.MD ```
.MD
.MD Install the OS dependencies:
.MD 
.MD ```
# Install OS dependencies
screencast paste begin
{
  yum -y install socat conntrack ipset wget
}
screencast paste end
.MD ```
.MD 
.MD > The socat binary enables support for the `kubectl port-forward` command.
.MD 
.MD ### Disable Swap
.MD 
.MD By default the kubelet will fail to start if [swap](https://help.ubuntu.com/community/SwapFaq) is enabled. It is [recommended](https://github.com/kubernetes/kubernetes/issues/7294) that swap be disabled to ensure Kubernetes can provide proper resource allocation and quality of service.
.MD 
.MD Verify if swap is enabled:
.MD 
.MD ```
# Is swap on?
swapon --show
# Yes, and so it should be for a laptop.
.MD ```
.MD 
.MD If output is empty then swap is not enabled. If swap is enabled run the following command to disable swap immediately:
.MD 
.MD ```
# `mokctl` uses a kubeadm configuration file to get components to ignore swap.
# For this lab the following lines will trick kubernetes into
# thinking that swap is off instead:
touch /swapoff
mount --bind /swapoff /proc/swaps
# We don't want to turn swap off on our laptop.
.MD ```
.MD 
.MD Verify again if swap is enabled:
.MD 
.MD ```
# Is swap on?
swapon --show
# Seems to be off now :)
.MD ```
.MD 
.MD Swap should now return no output, meaning, it's off.
.MD 
.MD > Swap is not actually turned off, since you don't want to do that on your laptop.
.MD

# Configure /etc/hosts

.MD ### Configure the `/etc/hosts` file
.MD 
screencast delay 8
.MD We need to add name-to-IP DNS resolution for the 'nodes' in the cluster. Adding the mappings to the `/etc/hosts` file is a simple way to do this. Another way would be to add DNS entries to a DNS server on your local network.
screencast delay 18
.MD
.MD Set some variables to hold the masters IP addresses:
.MD
.MD ```
screencast sleep 1
# Set master IP variables
screencast sleep 1
screencast paste
MASTER1=$(grep kthw-master-1 /root/cluster-list.txt | awk '{ print $NF; }')
echo $MASTER1
screencast paste
MASTER2=$(grep kthw-master-2 /root/cluster-list.txt | awk '{ print $NF; }')
echo $MASTER2
screencast paste
MASTER3=$(grep kthw-master-3 /root/cluster-list.txt | awk '{ print $NF; }')
echo $MASTER3
screencast sleep 1
.MD ```
.MD
.MD Set some variables to hold the workers IP addresses:
.MD
.MD ```
screencast sleep 1
# Set worker IP variables
screencast paste
WORKER1=$(grep kthw-worker-1 /root/cluster-list.txt | awk '{ print $NF; }')
echo $WORKER1
screencast paste
WORKER2=$(grep kthw-worker-2 /root/cluster-list.txt | awk '{ print $NF; }')
echo $WORKER2
screencast paste
WORKER3=$(grep kthw-worker-3 /root/cluster-list.txt | awk '{ print $NF; }')
echo $WORKER3
screencast sleep 1
.MD ```
.MD
.MD Add names to `/etc/hosts`:
.MD
.MD ```
# write /etc/hosts
screencast paste begin
{
cat <<EnD | tee -a /etc/hosts
$MASTER1 kthw-master-1
$MASTER2 kthw-master-2
$MASTER3 kthw-master-3
$WORKER1 kthw-worker-1
$WORKER2 kthw-worker-2
$WORKER3 kthw-worker-3
EnD
}
screencast paste end
.MD ```
.MD

.MD ### Download and Install Worker Binaries
.MD 
.MD ```
# Download the worker binaries
screencast waitforsilence
screencast paste begin
wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.15.0/crictl-v1.15.0-linux-amd64.tar.gz \
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc8/runc.amd64 \
  https://github.com/containernetworking/plugins/releases/download/v0.8.2/cni-plugins-linux-amd64-v0.8.2.tgz \
  https://github.com/containerd/containerd/releases/download/v1.2.9/containerd-1.2.9.linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.15.3/bin/linux/amd64/kubelet
screencast paste end
.MD ```
.MD 
.MD Create the installation directories:
.MD 
.MD ```
# Create the installation directories:
screencast paste begin
mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
screencast paste end
.MD ```
.MD 
.MD Install the worker binaries:
.MD 
.MD ```
# Install the worker binaries:
screencast paste begin
{
  mkdir containerd
  tar -xvf crictl-v1.15.0-linux-amd64.tar.gz
  tar -xvf containerd-1.2.9.linux-amd64.tar.gz -C containerd
  tar -xvf cni-plugins-linux-amd64-v0.8.2.tgz -C /opt/cni/bin/
  mv runc.amd64 runc
  chmod +x crictl kubectl kube-proxy kubelet runc 
  mv -f crictl kubectl kube-proxy kubelet runc /usr/local/bin/
  mv -f containerd/bin/* /bin/
}
screencast paste end
.MD ```
.MD 
.MD ### Configure CNI Networking
.MD 
.MD Work out the Pod CIDR range for the current compute instance.
.MD 
.MD We will use the following subnets from the 10.200.0.0/16 range:
.MD
.MD *  10.200.1.0/24 for kthw-worker-1
.MD *  10.200.2.0/24 for kthw-worker-2
.MD *  10.200.3.0/24 for kthw-worker-3
.MD
.MD ```
# Work out the pod subnet for this host:
screencast paste
POD_CIDR="10.200.$(hostname -s | grep -o '.$').0/24"
echo $POD_CIDR
.MD ```
.MD 
.MD Create the `bridge` network configuration file:
.MD 
.MD ```
# Write the CNI bridge configuration:
screencast paste begin
cat <<EOF | tee /etc/cni/net.d/10-bridge.conf
{
    "cniVersion": "0.3.1",
    "name": "bridge",
    "type": "bridge",
    "bridge": "cnio0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "${POD_CIDR}"}]
        ],
        "routes": [{"dst": "0.0.0.0/0"}]
    }
}
EOF
screencast paste end
.MD ```
.MD 
.MD Create the `loopback` network configuration file:
.MD 
.MD ```
# Write the CNI loopback device config:
screencast paste begin
cat <<EOF | tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.3.1",
    "name": "lo",
    "type": "loopback"
}
EOF
screencast paste end
.MD ```
.MD 
.MD ### Configure containerd
.MD 
.MD See also: [Docker Storage Drivers](https://docs.docker.com/storage/storagedriver/select-storage-driver/).
.MD
.MD Create the `containerd` configuration file:
.MD 
.MD ```
# Containerd config:
screencast paste
mkdir -p /etc/containerd/
.MD ```
.MD 
.MD Write a default configuration using the 'native' storage driver:
.MD
.MD ```
# Overlayfs won't work, use VFS (called 'native' now):
screencast paste begin
containerd config default | sed 's/overlayfs/native/' >/etc/containerd/config.toml
screencast paste end
.MD ```
.MD 
.MD Create the `containerd.service` systemd unit file:
.MD 
.MD ```
# Create the systemd unit file:
screencast paste begin
cat <<EOF | tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF
screencast paste end
.MD ```
.MD 
.MD ### Configure the Kubelet
.MD 
.MD ```
# Configure the kubelet:
screencast paste begin
{
  mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
  mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
  mv ca.pem /var/lib/kubernetes/
}
screencast paste end
.MD ```
.MD 
.MD Create the `kubelet-config.yaml` configuration file:
.MD 
.MD ```
# Kubelet config:
screencast paste begin
cat <<EOF | tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
podCIDR: "${POD_CIDR}"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
EOF
screencast paste end
.MD ```
.MD 
.MD > The `resolvConf` configuration is used to avoid loops when using CoreDNS for service discovery on systems running `systemd-resolved`. 
.MD 
.MD Create the `kubelet.service` systemd unit file:
.MD 
.MD ```
# Create systemd unit file for kubelet:
screencast paste begin
cat <<EOF | tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
screencast paste end
.MD ```
.MD 
.MD ### Configure the Kubernetes Proxy
.MD 
# Configure the kube-proxy:
.MD ```
screencast paste
mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
.MD ```
.MD 
.MD Create the `kube-proxy-config.yaml` configuration file:
.MD 
.MD ```
# Write the kube-proxy-config.yaml file:
screencast paste begin
cat <<EOF | tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.200.0.0/16"
EOF
.MD ```
.MD 
.MD Create the `kube-proxy.service` systemd unit file:
.MD 
.MD ```
# Write the systemd unit file for kube-proxy:
screencast paste begin
cat <<EOF | tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
screencast paste end
.MD ```
.MD 
.MD ### Start the Worker Services
.MD 
.MD ```
# Start worker services:
screencast paste begin
{
  systemctl daemon-reload
  systemctl enable containerd kubelet kube-proxy
  systemctl start containerd kubelet kube-proxy
}
screencast paste end
.MD ```
.MD 
.MD > Remember to run the above commands on each worker node: `kthw-worker-1`, `kthw-worker-2`, and `kthw-worker-3`.
.MD 
.MD ## Verification
.MD 
.MD > The compute instances created in this tutorial will not have permission to complete this section. Run the following commands from one of the master nodes.
.MD 
.MD Log out of the workers:
.MD
.MD ```
# Log out of the workers:
exit
exit
.MD ```
.MD
.MD Log in to one of the master nodes to run kubectl:
.MD
.MD ```
# And log in to kthw-master-1
screencast paste
mokctl exec kthw-master-1
cd    # <- our admin.kubeconfig is in root's home (/root)
.MD ```
.MD
.MD List the registered Kubernetes nodes:
.MD 
.MD ```
# Verification - list the nodes:
screencast paste
kubectl get nodes --kubeconfig admin.kubeconfig
# It worked!
.MD ```
.MD 
.MD > output
.MD 
.MD ```
.MD NAME            STATUS   ROLES    AGE   VERSION
.MD kthw-worker-1   Ready    <none>   15s   v1.15.3
.MD kthw-worker-2   Ready    <none>   15s   v1.15.3
.MD kthw-worker-3   Ready    <none>   15s   v1.15.3
.MD ```
# Log out of the master container
.MD
.MD Log out of the master container:
.MD
.MD ```
exit
.MD ```
.MD

# All done :)
.MD

# -------------------------------------------
# Next: Configuring kubectl for Remote Access
screencast paste
# -------------------------------------------
.MD Next: [Configuring kubectl for Remote Access](kthw-10.md)
.TS ```
